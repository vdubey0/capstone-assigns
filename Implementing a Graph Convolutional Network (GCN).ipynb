{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9495115280151367\n",
      "Epoch 10, Loss: 0.5894498229026794\n",
      "Epoch 20, Loss: 0.0932442769408226\n",
      "Epoch 30, Loss: 0.019700631499290466\n",
      "Epoch 40, Loss: 0.007515148725360632\n",
      "Epoch 50, Loss: 0.0043111443519592285\n",
      "Epoch 60, Loss: 0.003122517140582204\n",
      "Epoch 70, Loss: 0.0025340530555695295\n",
      "Epoch 80, Loss: 0.002172798151150346\n",
      "Epoch 90, Loss: 0.0019147110870108008\n",
      "Final Loss: 0.0017305193468928337\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch == 99:\n",
    "        print(f'Final Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cecfb-ce0b-4ee3-819c-a36637cea9f5",
   "metadata": {},
   "source": [
    "What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "\n",
    "My intuition is that we would increase accuracy (decrease loss) if we added more layers. This allows the network to capture more complex features in the data that would be beneficial in its classification. However, adding more layers would increase over-smoothing because the graph would aggregate information about particular nodes from more and more of the other nodes in the network. This means that the representation of each node would become much more similar because its representation is formed in a very similar manner to the rest of the nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b13c77-24c3-4086-a060-12756e25ec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9400514364242554\n",
      "Epoch 10, Loss: 0.9110769033432007\n",
      "Epoch 20, Loss: 0.26767370104789734\n",
      "Epoch 30, Loss: 0.05882469564676285\n",
      "Epoch 40, Loss: 0.012696413323283195\n",
      "Epoch 50, Loss: 0.004139997996389866\n",
      "Epoch 60, Loss: 0.002468880033120513\n",
      "Epoch 70, Loss: 0.0018266976112499833\n",
      "Epoch 80, Loss: 0.0014895839849486947\n",
      "Epoch 90, Loss: 0.0012793855275958776\n",
      "Final Loss: 0.00114056293386966\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "class GCN2Layer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(GCN2Layer, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN2Layer(input_dim=dataset.num_node_features, hidden_dim1=16, hidden_dim2=8, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch == 99:\n",
    "        print(f'Final Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2c173a-870c-4bdb-8d96-5a8af11e5081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39260296153576946"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00114056293386966 - 0.0018777881050482392) / 0.0018777881050482392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85308854-b62d-4055-8ff0-cc3126e2be67",
   "metadata": {},
   "source": [
    "By adding a 3rd layer with 8 nodes, we see a 39% decrease in loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499e39a-0c96-4665-ba5b-dd2a83bbfd66",
   "metadata": {},
   "source": [
    "What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "\n",
    "By adding a larger hidden dimension, I would expect the model's capacity to decrease. Increasing the hidden dimension size multiplicitly increases the number of parameters, I would expect the model to naturally overfit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0908481f-f970-4ba3-93ef-03a6d3c7a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9406793117523193\n",
      "Epoch 10, Loss: 0.07691950350999832\n",
      "Epoch 20, Loss: 0.0033538031857460737\n",
      "Epoch 30, Loss: 0.0006547874072566628\n",
      "Epoch 40, Loss: 0.0002803529496304691\n",
      "Epoch 50, Loss: 0.0001895413442980498\n",
      "Epoch 60, Loss: 0.00015777147200424224\n",
      "Epoch 70, Loss: 0.00014230006490834057\n",
      "Epoch 80, Loss: 0.00013241938722785562\n",
      "Epoch 90, Loss: 0.00012499105650931597\n",
      "Final Loss: 0.00011930983600905165\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "class GCNLargerHL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNLargerHL, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCNLargerHL(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch == 99:\n",
    "        print(f'Final Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e474d381-de78-43d1-bc42-a26a65391174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9364625669486885"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00011930983600905165 - 0.0018777881050482392) / 0.0018777881050482392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1595cd-4287-44c6-84b0-8c30a1458726",
   "metadata": {},
   "source": [
    "What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "I don't have too much intuition on this, but in the last assignment, my performance greatly increased when I used ReLU instead of sigmoid. So based on that, I expect maybe a similar thing here - switching to sigmoid would decrease performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b9a51ff-e304-402d-a97c-ff329dee6ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.044727325439453\n",
      "Epoch 10, Loss: 1.4465745687484741\n",
      "Epoch 20, Loss: 0.9942996501922607\n",
      "Epoch 30, Loss: 0.6476757526397705\n",
      "Epoch 40, Loss: 0.4087792634963989\n",
      "Epoch 50, Loss: 0.26063838601112366\n",
      "Epoch 60, Loss: 0.17464640736579895\n",
      "Epoch 70, Loss: 0.12472358345985413\n",
      "Epoch 80, Loss: 0.09445361793041229\n",
      "Epoch 90, Loss: 0.07492941617965698\n",
      "Final Loss: 0.06269379705190659\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "class GCNSigmoid(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNSigmoid, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCNSigmoid(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch == 99:\n",
    "        print(f'Final Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae08e42-5c23-4626-ac4c-1c955ac10041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.38704557950963"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.06269379705190659 - 0.0018777881050482392) / 0.0018777881050482392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54146a29-08e6-4828-b9f6-a9a1f3953ba4",
   "metadata": {},
   "source": [
    "We see a 32% increase in loss when shifting from ReLU to sigmoid. Mathematically, I'm not too sure why this is the case and what the reason behind this is, but maybe we can touch on this in discussions. It could be because we see deadzones at the top and bottom of the sigmoid function (where the input is either very small or very large), which isn't ideal for making differentiations in these extreme cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975a5c5",
   "metadata": {},
   "source": [
    "What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "\n",
    "Naturally, with less data, I would expect the performance of the model to decrease. Since we only train on 10% of the data, we capture much less information about the data, as a whole. Decreasing the data gives the model less examples from which it can draw conclusions/recognize patterns. I'd expect we'd see a pretty random performance on the test set because our training set probably wasn't comprehensive enough for the model to capture and apply its learned knowledge to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96066da7-dbde-4aa6-849c-702a6d53e781",
   "metadata": {},
   "source": [
    "What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55e1c4d0-b6ee-46e0-be6a-137457bc3b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9499214887619019\n",
      "Epoch 10, Loss: 0.0588170662522316\n",
      "Epoch 20, Loss: 0.019654853269457817\n",
      "Epoch 30, Loss: 0.010645276866853237\n",
      "Epoch 40, Loss: 0.006953630596399307\n",
      "Epoch 50, Loss: 0.00499807670712471\n",
      "Epoch 60, Loss: 0.003810119116678834\n",
      "Epoch 70, Loss: 0.00302361068315804\n",
      "Epoch 80, Loss: 0.0024691051803529263\n",
      "Epoch 90, Loss: 0.002059818943962455\n",
      "Final Loss: 0.001775294542312622\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    if epoch == 99:\n",
    "        print(f'Final Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66bb4487-fdc2-49f5-8129-06d75b8e7bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05458207050096537"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.001775294542312622 - 0.0018777881050482392) / 0.0018777881050482392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68242399-033c-48ef-8d25-c75cfc50e672",
   "metadata": {},
   "source": [
    "I didn't really have too much intuition on this question. However, changing from Adam to RMSProp decreased loss by around 5%. \n",
    "\n",
    "I did some research on the different optimizers. Regarding convergence speed, I found that typically Adam is faster because it adds a momentum component to the RMSProp algorithm. This allows for quicker convergence to a minimum. Because of this, I had expected that the switch RMSProp would have decrease performance because it would have taken more time to converge to the loss that Adam ended up finding. Maybe an increased number of epochs would show this. Or it just could be the case that RMSprop is simply better for this problem than Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3760e-55b9-4ce5-ab8e-44f0cfc63d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
