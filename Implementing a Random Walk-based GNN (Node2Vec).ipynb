{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdf2a864d5e4293bacf732567d05725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:26<00:00,  3.76it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n",
      "/tmp/ipykernel_2294/523172613.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9939231872558594\n",
      "Epoch 10, Loss: 1.2608762979507446\n",
      "Epoch 20, Loss: 0.90863436460495\n",
      "Epoch 30, Loss: 0.7604204416275024\n",
      "Epoch 40, Loss: 0.6880519390106201\n",
      "Epoch 50, Loss: 0.6468108892440796\n",
      "Epoch 60, Loss: 0.6196609735488892\n",
      "Epoch 70, Loss: 0.6001744866371155\n",
      "Epoch 80, Loss: 0.5853856205940247\n",
      "Epoch 90, Loss: 0.5735852718353271\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1353dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
