{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e674578a9a4eee959895db9cdc760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 25/25 [00:08<00:00,  3.11it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 25/25 [00:07<00:00,  3.18it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 25/25 [00:07<00:00,  3.13it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 25/25 [00:07<00:00,  3.14it/s]\n",
      "Generating walks (CPU: 5): 100%|██████████| 25/25 [00:08<00:00,  3.10it/s]\n",
      "Generating walks (CPU: 6): 100%|██████████| 25/25 [00:07<00:00,  3.15it/s]\n",
      "Generating walks (CPU: 7): 100%|██████████| 25/25 [00:07<00:00,  3.16it/s]\n",
      "Generating walks (CPU: 8): 100%|██████████| 25/25 [00:07<00:00,  3.16it/s]\n",
      "/tmp/ipykernel_160/2407542284.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9151047468185425\n",
      "Epoch 10, Loss: 1.2516547441482544\n",
      "Epoch 20, Loss: 0.9109947085380554\n",
      "Epoch 30, Loss: 0.757879912853241\n",
      "Epoch 40, Loss: 0.6823053359985352\n",
      "Epoch 50, Loss: 0.6378163695335388\n",
      "Epoch 60, Loss: 0.6071047782897949\n",
      "Epoch 70, Loss: 0.5842233896255493\n",
      "Epoch 80, Loss: 0.5664762258529663\n",
      "Epoch 90, Loss: 0.5522255301475525\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=8) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570f311-16b0-489e-bcb1-75b1e6aaf718",
   "metadata": {},
   "source": [
    "What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "\n",
    "My intuition is that increasing the number of walks would increase the amount of time to generate the embeddings. This is because the algorithm will increase the amount of walks it starts from each node. The embeddings will be more comprehensive though because the algorithm will capture more of the neighborhood of the root node. It can better generate vectors because it will have a more thorough view of a node's neighbors.\n",
    "\n",
    "I think that increasing the number of walks would increase model performance because of the improvement of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63608fb2-847d-4c3a-a56a-ffa02139a532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa71cdd5d5f94749bf8e135aee41f4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 250/250 [01:22<00:00,  3.04it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 250/250 [01:23<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9552465677261353\n",
      "Epoch 10, Loss: 1.2400709390640259\n",
      "Epoch 20, Loss: 0.9010287523269653\n",
      "Epoch 30, Loss: 0.7543397545814514\n",
      "Epoch 40, Loss: 0.6809619069099426\n",
      "Epoch 50, Loss: 0.6386410593986511\n",
      "Epoch 60, Loss: 0.6100683808326721\n",
      "Epoch 70, Loss: 0.58897465467453\n",
      "Epoch 80, Loss: 0.5725937485694885\n",
      "Epoch 90, Loss: 0.5593566298484802\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=500, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235235d1-b29b-4883-bfbc-f670694d7b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02193464147669893"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5448827743530273 - 0.557102620601654) / 0.557102620601654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e957d9-e505-4894-b6ce-d8f0a0cae365",
   "metadata": {},
   "source": [
    "We see a 2% decrease in loss by increasing the num_walks from 200 to 500. I think a strong consideration for this parameter is balancing computation time and performance. It will be important to figure out when the increase in computation time outweights the small increase in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3daefd-0d5c-4065-abc1-a0af3575f7d9",
   "metadata": {},
   "source": [
    "What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "If we reduced the walk length, the structural information about the graph will be less. This is because the random walks won't go as deep into the graph so we will miss the information that is not in the immediate neighborhood of the root node. I think that doing this would reduce the performance of the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67dcbdc-368b-464b-ae82-9c433c4081fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e304e17ad734e2fb2e9695b419e8047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:16<00:00,  6.16it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:16<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.036820411682129\n",
      "Epoch 10, Loss: 1.2899699211120605\n",
      "Epoch 20, Loss: 0.9394242763519287\n",
      "Epoch 30, Loss: 0.7780355215072632\n",
      "Epoch 40, Loss: 0.6983869075775146\n",
      "Epoch 50, Loss: 0.6521328091621399\n",
      "Epoch 60, Loss: 0.620934247970581\n",
      "Epoch 70, Loss: 0.5978969931602478\n",
      "Epoch 80, Loss: 0.5800913572311401\n",
      "Epoch 90, Loss: 0.5658071041107178\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=15, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d71379-7b91-4921-b1e0-bf529cd7b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006475488333710831"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5534951090812683 - 0.557102620601654) / 0.557102620601654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee539087-697d-466f-990f-cd000e9972c7",
   "metadata": {},
   "source": [
    "We see a very slight decrease in loss by changing the walk length from 30 to 15. This is not what I expected but I guess in the context of the problem it makes sense. A paper's classification could be very dependent on its immediate neighborhood so changing the walk length is just a matter of extra noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e8f84-4937-4a9a-a678-7bb29dff1b56",
   "metadata": {},
   "source": [
    "What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "\n",
    "I think that we would lose a lot of the information in the graph because all the edges are now directional. A lot of edges that existed in the undirected graph are no more in the directed graph. Based on the nature of this problem, I think that having the bi-directional edges as in the undirected graphs is worthwhile. So I think that there would be a decrease in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1606a08-5c3c-481c-8765-dbc65d38be0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e149bd67af34038bd9ac74293b027cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:31<00:00,  3.15it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:32<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9904170036315918\n",
      "Epoch 10, Loss: 1.2779759168624878\n",
      "Epoch 20, Loss: 0.9268447160720825\n",
      "Epoch 30, Loss: 0.770454466342926\n",
      "Epoch 40, Loss: 0.6934294700622559\n",
      "Epoch 50, Loss: 0.648582398891449\n",
      "Epoch 60, Loss: 0.6187095046043396\n",
      "Epoch 70, Loss: 0.5969102382659912\n",
      "Epoch 80, Loss: 0.580097496509552\n",
      "Epoch 90, Loss: 0.5665943622589111\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=False)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b5e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01703768983711885"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5665943622589111 - 0.557102620601654) / 0.557102620601654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3aea1-de29-454e-93f4-c065e59ef1a7",
   "metadata": {},
   "source": [
    "What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "\n",
    "I'd assume that we'd see an increase in performance if we added more features. More features would mean more data for the embedding and modeling algorithm to learn from. It could make connections in the new data that could reveal better classifications. However, we would also tradeoff training time for both the embedding and modeling steps. There would be more computations that the algorithms would need to make, which would inherently slow down the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9b160-6a60-42ff-8e06-2d3bff1260fc",
   "metadata": {},
   "source": [
    "What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "\n",
    "I don't think that the classifier performance will change significantly. The nodes are mapped into a space where the more similar nodes have a more similar embedding. These embeddings won't change if there are more classes, so I think that the model will still be able to use them to differentiate between the extra classes. In other words, it should still map the more similar nodes to the same class most of the time, regardless of how many classes there are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c9d2a-fc15-4bb6-a5c0-3436000df133",
   "metadata": {},
   "source": [
    "What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f99a2c9-3067-48e2-804a-b8461a4f8692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192583f3b4ee48ac8ac94f4728a51197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:31<00:00,  3.18it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.007720708847046\n",
      "Epoch 10, Loss: 1.1136757135391235\n",
      "Epoch 20, Loss: 0.7734028100967407\n",
      "Epoch 30, Loss: 0.6548328399658203\n",
      "Epoch 40, Loss: 0.5966346263885498\n",
      "Epoch 50, Loss: 0.5577636361122131\n",
      "Epoch 60, Loss: 0.5290225148200989\n",
      "Epoch 70, Loss: 0.5069485902786255\n",
      "Epoch 80, Loss: 0.48921290040016174\n",
      "Epoch 90, Loss: 0.4743979275226593\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(128, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4bd4024-48c1-4d1a-b7ec-9ffcdc10c335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1484550422499829"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.4743979275226593 - 0.557102620601654) / 0.557102620601654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd675c5",
   "metadata": {},
   "source": [
    "By increasing the embedding dimension from 64 to 128, we see in 14% decrease in loss. However, the time to train was much longer. This is probably because we naturally have more numbers we need to work and hence more computations, which add extra time in computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092e07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
